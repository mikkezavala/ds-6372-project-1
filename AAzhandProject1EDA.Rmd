---
title: "Project 1 EDA"
author: "Arman Azhand"
date: "2025-01-29"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(visdat)
library(tidyverse)
library(corrplot)
library(GGally)
library(car)
library(ggcorrplot)
library(plyr)
library(gridExtra)
library(FNN)
library(rgl)
library(caret)

knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
```

# EDA

## Import Data
```{r}
hospitalData <- read.csv("./project_hospitals.csv", header = TRUE)
```

## Sanity check

### Summary of the Data Set
```{r}
summary(hospitalData)
```
### Finding empty values
```{r}
colSums(is.na(hospitalData))
```
#### Plot of missing data
```{r}
vis_miss(hospitalData)
```

Our data set `hospitalData` from the csv `HospitalDurations` contains 113 observations of 12 variables. We have no missing values for any variable, thus we do not need to impute for any observations. Out of the 12 variables, one of them is an identifier variable `ID`, and will not be used for predicting. The patient's length of stay `Lgth.of.Sty` is the variable we would like to predict for. Therefor, we have 10 potential explanatory variables and 1 variable we are trying to predict for.


### Visualizing Hospital Stay Distribution

## Data Cleaning
```{r}
cleanData = hospitalData %>% select(-ID) # remove ID
cleanData$Region = as.factor(cleanData$Region) # convert Region from int to factor
cleanData$Med.Sc.Aff = as.factor(cleanData$Med.Sc.Aff) # convert Med.Sc.Aff from int to factor
summary(cleanData) # sanity check

# revalue factors
cleanData$Region = revalue(cleanData$Region, c("1" = "NE", "2" = "NC", "3" = "S", "4" = "W"))
cleanData$Med.Sc.Aff = revalue(cleanData$Med.Sc.Aff, c("1" = "Yes", "2" = "No"))
summary(cleanData) # sanity check

# labels for graphs
cleanDataLabels = c("Length of Stay",
                    "Age",
                    "Infection Risk",
                    "Routine culturing ratio",
                    "Routine chest X-ray ratio",
                    "Number of beds",
                    "Medical School Affiliation",
                    "Region",
                    "Average Daily census",
                    "Number of nurses",
                    "Available facilities")
```
```{r}
hist(cleanData$Lgth.of.Sty, xlab = "Length of Stay (Days)", main = "Histogram of Patient's Length of Stay")
```
We see some slight right skew for `Lgth.of.Sty`.

## Initial Graphs
```{r}
# correlation matrix
cor_matrix = cor(cleanData %>% select(-Med.Sc.Aff, -Region)) # removed categorial variables

# correlation plot
# corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
ggcorrplot(cor_matrix, lab = TRUE)
```
Length of Stay correlated more strongly with: + Infection Risk, + Average Patients, + Number of Beds.

### Correlograms with log transformations
```{r}
corData = cleanData %>% select(-Med.Sc.Aff, -Region)
lin_logCorData = log(corData)
lin_logCorData$Lgth.of.Sty = corData$Lgth.of.Sty
log_linCorData = corData %>% select(-Lgth.of.Sty)
log_linCorData$logLgth.of.Sty = log(corData$Lgth.of.Sty)


correlation_matrix = cor(corData, method = "pearson")
ggcorrplot(correlation_matrix, lab = TRUE, title = "Linear-Linear")

lin_log_correlation_matrix = cor(lin_logCorData, method = "pearson")
ggcorrplot(lin_log_correlation_matrix, lab = TRUE, title = "Linear-Log")

log_lin_correlation_matrix = cor(log_linCorData, method = "pearson")
ggcorrplot(log_lin_correlation_matrix, lab = TRUE, title = "Log-Linear")
```


```{r}
pairs(cleanData, panel = function(x, y) {
  points(x, y, pch = 16, col = "blue")  # Scatter points
  abline(lm(y ~ x), col = "red")  # Add trend line
})
```

```{r}
ggpairs(cleanData, lower = list(continuous = wrap("smooth", method = "lm", color = "blue")))
```
### Length of Stay vs other continuous variables
```{r}
for (i in c(2:6,9:11)) {
  tempPlot = ggplot(data=cleanData,aes(x=cleanData[,i],y=Lgth.of.Sty)) +
    geom_point() +
    geom_smooth() +
    ggtitle(paste0(cleanDataLabels[i]," vs Length of Stay")) +
    xlab(cleanDataLabels[i]) +
    ylab("Length of Stay (Days)") +
    theme(legend.position = "none")
  print(tempPlot)
}
```
### Length of Stay vs log of other continuous variables
```{r}
for (i in c(2:6,9:11)) {
  tempPlot = ggplot(data=cleanData,aes(x=log(cleanData[,i]),y=Lgth.of.Sty)) +
    geom_point() +
    geom_smooth() +
    ggtitle(paste0("Log ", cleanDataLabels[i]," vs Length of Stay")) +
    xlab(paste0("log ", cleanDataLabels[i])) +
    ylab("Length of Stay (Days)") +
    theme(legend.position = "none")
  print(tempPlot)
}
```

### log Length of Stay vs other continuous variables
```{r}
for (i in c(2:6,9:11)) {
  tempPlot = ggplot(data=cleanData,aes(x=log(cleanData[,i]),y=log(Lgth.of.Sty))) +
    geom_point() +
    geom_smooth() +
    ggtitle(paste0("Log ", cleanDataLabels[i]," vs Length of Stay")) +
    xlab(cleanDataLabels[i]) +
    ylab("log Length of Stay (Days)") +
    theme(legend.position = "none")
  print(tempPlot)
}
```

### Categorical grouped by Categorical vs Length of Stay
```{r}
ggplot(data=cleanData,aes(x=Region,y=Lgth.of.Sty,colour=Med.Sc.Aff)) +
  geom_boxplot() +
  ggtitle("Boxplot of Region vs Patient Length of Stay", "Grouped by Medical School Affiliation")
ggplot(data=cleanData,aes(x=Med.Sc.Aff,y=Lgth.of.Sty,colour=Region)) +
  geom_boxplot() +
  ggtitle("Boxplot of Medical School Affiliation vs Patient Length of Stay", "Grouped by Region")
```
By looking at the distributions (via the boxplots), we can see that there are some differences between a patient's length of stay if we group by Region (or by Medical School Affiliation).

**If grouping by Medical school affiliation:  **
Length of stay between the regions is fairly similar on average if there is an affiliation for medical school, but the distributions are much more different between the regions. If there is no affiliation, then the distributions are a bit more similar, but the averages are different between each region.

**If grouping by Region:  **
Length of stay on average tends to be similar between the regions (being slightly less in the W and S regions) with NE having a wider distribution of length of stay. Interesting to note that medical school affiliation tends to have lower length of stay if the affiliation is none.

### Continuous grouped by Categorical vs Length of Stay

#### Grouped by Region
```{r}
for (i in c(2:6,9:11)) {
  tempPlot = ggplot(data=cleanData,aes(x=cleanData[,i],y=Lgth.of.Sty,colour=Region)) +
    geom_point() +
    geom_smooth() +
    ggtitle(paste0(cleanDataLabels[i]," vs Length of Stay"), "Grouped by Region") +
    xlab(cleanDataLabels[i]) +
    ylab("Length of Stay (Days)") +
    theme(legend.position = "none")
  print(tempPlot)
}
```
*Will need to look at these a bit more in detail to see if we will want to include any interaction terms.*

#### Grouped by Medical School Affiliation
```{r}
for (i in c(2:6,9:11)) {
  tempPlot = ggplot(data=cleanData,aes(x=cleanData[,i],y=Lgth.of.Sty,colour=Med.Sc.Aff)) +
    geom_point() +
    geom_smooth() +
    ggtitle(paste0(cleanDataLabels[i]," vs Length of Stay"), "Grouped by Medical School Affiliation") +
    xlab(cleanDataLabels[i]) +
    ylab("Length of Stay (Days)") +
    theme(legend.position = "none")
  print(tempPlot)
}
```

# Objective 1
```{r}
allVarsModel = lm(Lgth.of.Sty ~ ., data = cleanData)

par(mfrow = c(2,2))
plot(allVarsModel)
par(mfrow = c(1,1))

summary(allVarsModel)
```

## VIFs
```{r}
vif(allVarsModel)
```
Number of Beds and Average Patients have *extremely* high VIFs, suggesting high multicollinearity. This makes sense, as the more room a hospital has (number of beds) then the more patients they can have on average. Average number of full time nurses has potential collinearity as well, which also makes sense as a hospital with more beds will likely have more nursing staff to take care of the patients.

```{r}
tempModel = lm(Lgth.of.Sty ~ Age + Inf.Risk + R.Cul.Rat + R.CX.ray.Rat + Avg.Nur + Pct.Ser.Fac + Med.Sc.Aff + Region, data = cleanData)

par(mfrow = c(2,2))
plot(tempModel)
par(mfrow = c(1,1))

summary(tempModel)
vif(tempModel)
```

```{r}
tempModel2 = lm(Lgth.of.Sty ~ Age + Inf.Risk + Med.Sc.Aff + Region, data = cleanData)

par(mfrow = c(2,2))
plot(tempModel2)
par(mfrow = c(1,1))

summary(tempModel2)
vif(tempModel2)
```
```{r}
train_control <- trainControl(method = "cv", number = 5)
set.seed(1234)  # Set a seed for reproducibility
knn_model <- train(
  Lgth.of.Sty ~ .,                  # Formula: regress Lgth.of.Stay on all predictors
  data = cleanData,         # Dataset containing only numeric predictors
  method = "knn",              # Specify k-Nearest Neighbors as the model
  trControl = train_control,   # 10-fold cross-validation
  tuneGrid = expand.grid(k=c(1:10,20,30))  # Grid of k values to explore
)

print(knn_model)
plot(knn_model)
print(knn_model$bestTune)
```
When looking at all numeric predictors, the best k for RMSE is 30, however, we see that at k = 9, we also have quite a low RMSE as well.

```{r}
train_control <- trainControl(method = "cv", number = 5)
set.seed(1234)  # Set a seed for reproducibility
knn_model <- train(
  Lgth.of.Sty ~ Age + Inf.Risk + R.Cul.Rat + R.CX.ray.Rat + N.Beds + Med.Sc.Aff,                  # Formula: regress Lgth.of.Stay on predictors
  data = cleanData,         # Dataset containing only numeric predictors
  method = "knn",              # Specify k-Nearest Neighbors as the model
  trControl = train_control,   # 10-fold cross-validation
  tuneGrid = expand.grid(k=c(1:10,20,30))  # Grid of k values to explore
)

print(knn_model)
plot(knn_model)
print(knn_model$bestTune)
```
Similar to looking at all predictors, for the predictors our LASSO model chose, we see again that at k = 30, our RMSE is lowest, however, at k = 9 we have a low RMSE as well.

```{r}
train_control <- trainControl(method = "cv", number = 5)
set.seed(1234)  # Set a seed for reproducibility
knn_model <- train(
  Lgth.of.Sty ~ Age + Inf.Risk + Region + Med.Sc.Aff, # Formula: regress Lgth.of.Stay on predictors
  data = cleanData,         # Dataset containing only numeric predictors
  method = "knn",              # Specify k-Nearest Neighbors as the model
  trControl = train_control,   # 10-fold cross-validation
  tuneGrid = expand.grid(k=c(1:10,20,30))  # Grid of k values to explore
)

print(knn_model)
plot(knn_model)
print(knn_model$bestTune)
```
Interestingly, for the predictors of Age, Inf.Risk, Med.Sc.Aff, and Region, k = 10 has the best RMSE.
